parameters:
  num_iterations: 30 #number of total runs
  training_interval: 0 #not used
  num_episodes: 16 #30 #number of n_parallel*epiosode iterations (each creates n_parallel episodes) <--- N-Batches
  num_steps: 0
  num_unroll_steps: 5  
  num_simulations: 21 #in MCTS
  epochs: 5
  d_max: 0 #max search depth
  actions: [0, 1, 2]
  minibatch_size: 128 #depends on num_episodes
  discount_factor: 0.98
  latent_resolution: [4, 4]
  real_resolution: [16, 16]
  n_parallel: 32 #should be 32

  replay_buffer_max: 7000 #512 * 7 (we create 16*32=512 episodes per iter)

  load_weights: False
  checkpoint_path: "weights/checkpt1.pth"

  search:
    mcts_name: "MCTSSearchVec" #"MCTSSearch"
    c1: 1.25
    c2: 19652.0  
    discount_factor: 0.98

  model:
    learning_rate: 0.001
    agent_name: "MuZeroAgent"
    num_supports: 11 #for support integers in [-300, 300] (paper page 14)
    supports_min: -5
    supports_max: 5
    latent_channels: [128, 256] #number of channels we use in the latent space 
    state_history_length: 20 #Should be at least 20 - this is the number of steps it takes to see a whole game from start to finish (requires padding history-len)
    device: "cuda"
    latent_resolution: [4, 4] #Redundant

    representation_network:
      num_res_blocks: [1, 1, 1]
      activation: "relu"
      learning_rate: 0.001

    dynamics_network:
      num_res_blocks: 3
      activation: "relu"
      learning_rate: 0.001

    prediction_network:
      num_res_blocks: 3
      num_actions: 3
      activation: "relu" #activation for conv block   
      learning_rate: 0.001

  environment:
    # environment_name: "BreakoutEnvironment"
    environment_name: "BreakoutEnvironment"
    # environment_path: "environment.breakout"
    environment_path: "environment.parallel_breakout"
    resolution: [16, 16] #[height, width]
    brick_rows: 5
    n_parallel: 32
    paddle_hit_reward: 1.0
    brick_hit_reward: 1.0
    game_lost_reward: -2.0
    game_won_reward: 5.0
